{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization"
      ],
      "metadata": {
        "id": "SCLga4jHMSjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the Environment"
      ],
      "metadata": {
        "id": "eR27euuTMerj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!java --version\n",
        "!python --version"
      ],
      "metadata": {
        "id": "FAqUDKAkMt_Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35c94e23-1c7e-4098-a0f1-4606c17e3d9f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk 11.0.18 2023-01-17\n",
            "OpenJDK Runtime Environment (build 11.0.18+10-post-Ubuntu-0ubuntu120.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.18+10-post-Ubuntu-0ubuntu120.04.1, mixed mode, sharing)\n",
            "Python 3.9.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Apache Spark (PySpark)"
      ],
      "metadata": {
        "id": "Yrnt72iGMhUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "pPn_WTO3MzBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1762830-7f61-4d60-9ffa-fbd15871d2a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824028 sha256=4878912ee1680ed27ad94cec334ad7035aaa06345d0226acb871f80348ad60ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Apache Spark context"
      ],
      "metadata": {
        "id": "o-Eqrng2M3yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Apache Spark SQL\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark Session/Context\n",
        "# We are using local machine with all the CPU cores [*]\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Hello Pyspark\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "KxFUGZy1M9NV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check spark session\n",
        "print(spark)"
      ],
      "metadata": {
        "id": "RVW4AfDkGnMy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85c005b5-af5a-4d20-fe63-f8684fa76310"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7f8108054400>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Mining Task\n",
        "\n",
        "The LSH task always consists of three steps:\n",
        "\n",
        "\n",
        "1. Converting original data into vectors\n",
        "2. Calculate the hash using MinHash algorithm\n",
        "3. Searching the similar pairs using k-Nearest Neighbor, or join algorithm.\n",
        "\n"
      ],
      "metadata": {
        "id": "rTqJYLL3MTlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the dataset"
      ],
      "metadata": {
        "id": "fJKgQ1vxehDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "snRNGVHXMc2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62414060-69f2-45d7-cb87-a02b748cc856"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (1.5.13)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle/\n",
        "!touch ~/.kaggle/kaggle.json\n",
        "\n",
        "# PLEASE USE YOUR OWN KEY\n",
        "# Download your own key according to this instruction https://github.com/Kaggle/kaggle-api#api-credentials\n",
        "\n",
        "api_token = {\"username\":\"oz471420\",\"key\":\"c62ace22e73f19fdda98c399245acf25\"}\n",
        "\n",
        "import json\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "bCfvcmJkOh90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82ce0c1f-35e2-460f-8434-6fed5db580ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle/’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download from https://www.kaggle.com/datasets/urbanbricks/wikipedia-promotional-articles\n",
        "\n",
        "!kaggle datasets download -d urbanbricks/wikipedia-promotional-articles"
      ],
      "metadata": {
        "id": "GsPLp0NTPllf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76b59388-9fbd-4408-b35f-6fe99960b8b5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading wikipedia-promotional-articles.zip to /content\n",
            " 94% 189M/201M [00:02<00:00, 94.8MB/s]\n",
            "100% 201M/201M [00:02<00:00, 89.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip wikipedia-promotional-articles.zip"
      ],
      "metadata": {
        "id": "Vc3PbKwXQRoU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de89cfcd-4ed7-48da-ee0e-cefdfd5a3c64"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  wikipedia-promotional-articles.zip\n",
            "  inflating: good.csv                \n",
            "  inflating: promotional.csv         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la"
      ],
      "metadata": {
        "id": "R4Lyz6kMOMrZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a52c2767-7f06-48c3-b426-9950107fe219"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 783144\n",
            "drwxr-xr-x 1 root root      4096 Apr  7 11:54 .\n",
            "drwxr-xr-x 1 root root      4096 Apr  7 11:47 ..\n",
            "drwxr-xr-x 4 root root      4096 Apr  5 13:29 .config\n",
            "-rw-r--r-- 1 root root 475685227 Oct 27  2019 good.csv\n",
            "-rw-r--r-- 1 root root 115360355 Oct 27  2019 promotional.csv\n",
            "drwxr-xr-x 1 root root      4096 Apr  5 13:30 sample_data\n",
            "-rw-r--r-- 1 root root 210863294 Apr  7 11:53 wikipedia-promotional-articles.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the dataset"
      ],
      "metadata": {
        "id": "LGe47j_-ercu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read CSV\n",
        "df = spark.read.option(\"header\", True).csv(\"/content/good.csv\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "ymmgwA1eSaYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d6dae5-e6ab-4559-c3d6-348171ce2799"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- url: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add an ID for the dataset\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "newsDF = df.withColumn(\"id\", monotonically_increasing_id())\n",
        "newsDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPESq0DI83oT",
        "outputId": "4578286c-bbbf-4873-f1bf-7c6d36b4e1a9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+---+\n",
            "|                text|                 url| id|\n",
            "+--------------------+--------------------+---+\n",
            "|Nycticebus linglo...|https://en.wikipe...|  0|\n",
            "|Oryzomys pliocaen...|https://en.wikipe...|  1|\n",
            "|.hack dt hk is a ...|https://en.wikipe...|  2|\n",
            "|The You Drive Me ...|https://en.wikipe...|  3|\n",
            "|0 8 4 is the seco...|https://en.wikipe...|  4|\n",
            "|I Corps is a corp...|https://en.wikipe...|  5|\n",
            "|The 1982 Florida ...|https://en.wikipe...|  6|\n",
            "|Tropical Depressi...|https://en.wikipe...|  7|\n",
            "|Tropical Depressi...|https://en.wikipe...|  8|\n",
            "|Tropical Depressi...|https://en.wikipe...|  9|\n",
            "|On 1 November 194...|https://en.wikipe...| 10|\n",
            "|1 1 is a song rec...|https://en.wikipe...| 11|\n",
            "|1 Thing is a song...|https://en.wikipe...| 12|\n",
            "|One Times Square,...|https://en.wikipe...| 13|\n",
            "|1 vs. 100 is an A...|https://en.wikipe...| 14|\n",
            "|One World Trade C...|https://en.wikipe...| 15|\n",
            "|The A1 in London ...|https://en.wikipe...| 16|\n",
            "|Tropical Depressi...|https://en.wikipe...| 17|\n",
            "|1 Nenokkadine Eng...|https://en.wikipe...| 18|\n",
            "|1 54 is an annual...|https://en.wikipe...| 19|\n",
            "+--------------------+--------------------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total rows\n",
        "newsDF.count()"
      ],
      "metadata": {
        "id": "IIZN5Ejkh3w7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27483ce5-debe-4929-a68b-a0905d8f3b7d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30279"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Prepare the tokenizer\n",
        "\n",
        "We transform the input into tokenized words."
      ],
      "metadata": {
        "id": "T5MC7DdtT8MQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the tokenizer\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "wordsDF = tokenizer.transform(newsDF)\n",
        "\n",
        "wordsDF.show()"
      ],
      "metadata": {
        "id": "pLXIayWqS_3O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f7f82ff-8bd2-4094-dc47-4c4b44a29dae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+---+--------------------+\n",
            "|                text|                 url| id|               words|\n",
            "+--------------------+--------------------+---+--------------------+\n",
            "|Nycticebus linglo...|https://en.wikipe...|  0|[nycticebus, ling...|\n",
            "|Oryzomys pliocaen...|https://en.wikipe...|  1|[oryzomys, plioca...|\n",
            "|.hack dt hk is a ...|https://en.wikipe...|  2|[.hack, dt, hk, i...|\n",
            "|The You Drive Me ...|https://en.wikipe...|  3|[the, you, drive,...|\n",
            "|0 8 4 is the seco...|https://en.wikipe...|  4|[0, 8, 4, is, the...|\n",
            "|I Corps is a corp...|https://en.wikipe...|  5|[i, corps, is, a,...|\n",
            "|The 1982 Florida ...|https://en.wikipe...|  6|[the, 1982, flori...|\n",
            "|Tropical Depressi...|https://en.wikipe...|  7|[tropical, depres...|\n",
            "|Tropical Depressi...|https://en.wikipe...|  8|[tropical, depres...|\n",
            "|Tropical Depressi...|https://en.wikipe...|  9|[tropical, depres...|\n",
            "|On 1 November 194...|https://en.wikipe...| 10|[on, 1, november,...|\n",
            "|1 1 is a song rec...|https://en.wikipe...| 11|[1, 1, is, a, son...|\n",
            "|1 Thing is a song...|https://en.wikipe...| 12|[1, thing, is, a,...|\n",
            "|One Times Square,...|https://en.wikipe...| 13|[one, times, squa...|\n",
            "|1 vs. 100 is an A...|https://en.wikipe...| 14|[1, vs., 100, is,...|\n",
            "|One World Trade C...|https://en.wikipe...| 15|[one, world, trad...|\n",
            "|The A1 in London ...|https://en.wikipe...| 16|[the, a1, in, lon...|\n",
            "|Tropical Depressi...|https://en.wikipe...| 17|[tropical, depres...|\n",
            "|1 Nenokkadine Eng...|https://en.wikipe...| 18|[1, nenokkadine, ...|\n",
            "|1 54 is an annual...|https://en.wikipe...| 19|[1, 54, is, an, a...|\n",
            "+--------------------+--------------------+---+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the dataset\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "vocabSize=1000\n",
        "\n",
        "# Train the CountVectorizer Model using our data\n",
        "cvModel = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=vocabSize, minDF=10).fit(wordsDF)\n",
        "\n",
        "# Transform our data into vector\n",
        "vectorizedDF = cvModel.transform(wordsDF)\n",
        "vectorizedDF.show()"
      ],
      "metadata": {
        "id": "v3imdzxvU9Gq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac6504e4-8010-4021-9f54-e27e42a61d10"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+---+--------------------+--------------------+\n",
            "|                text|                 url| id|               words|            features|\n",
            "+--------------------+--------------------+---+--------------------+--------------------+\n",
            "|Nycticebus linglo...|https://en.wikipe...|  0|[nycticebus, ling...|(1000,[0,1,2,3,4,...|\n",
            "|Oryzomys pliocaen...|https://en.wikipe...|  1|[oryzomys, plioca...|(1000,[0,1,2,3,4,...|\n",
            "|.hack dt hk is a ...|https://en.wikipe...|  2|[.hack, dt, hk, i...|(1000,[0,1,2,3,4,...|\n",
            "|The You Drive Me ...|https://en.wikipe...|  3|[the, you, drive,...|(1000,[0,1,2,3,4,...|\n",
            "|0 8 4 is the seco...|https://en.wikipe...|  4|[0, 8, 4, is, the...|(1000,[0,1,2,3,4,...|\n",
            "|I Corps is a corp...|https://en.wikipe...|  5|[i, corps, is, a,...|(1000,[0,1,2,3,4,...|\n",
            "|The 1982 Florida ...|https://en.wikipe...|  6|[the, 1982, flori...|(1000,[0,1,2,3,4,...|\n",
            "|Tropical Depressi...|https://en.wikipe...|  7|[tropical, depres...|(1000,[0,1,2,3,4,...|\n",
            "|Tropical Depressi...|https://en.wikipe...|  8|[tropical, depres...|(1000,[0,1,2,3,4,...|\n",
            "|Tropical Depressi...|https://en.wikipe...|  9|[tropical, depres...|(1000,[0,1,2,3,4,...|\n",
            "|On 1 November 194...|https://en.wikipe...| 10|[on, 1, november,...|(1000,[0,1,2,3,4,...|\n",
            "|1 1 is a song rec...|https://en.wikipe...| 11|[1, 1, is, a, son...|(1000,[0,1,2,3,4,...|\n",
            "|1 Thing is a song...|https://en.wikipe...| 12|[1, thing, is, a,...|(1000,[0,1,2,3,4,...|\n",
            "|One Times Square,...|https://en.wikipe...| 13|[one, times, squa...|(1000,[0,1,2,3,4,...|\n",
            "|1 vs. 100 is an A...|https://en.wikipe...| 14|[1, vs., 100, is,...|(1000,[0,1,2,3,4,...|\n",
            "|One World Trade C...|https://en.wikipe...| 15|[one, world, trad...|(1000,[0,1,2,3,4,...|\n",
            "|The A1 in London ...|https://en.wikipe...| 16|[the, a1, in, lon...|(1000,[0,1,2,3,4,...|\n",
            "|Tropical Depressi...|https://en.wikipe...| 17|[tropical, depres...|(1000,[0,1,2,3,4,...|\n",
            "|1 Nenokkadine Eng...|https://en.wikipe...| 18|[1, nenokkadine, ...|(1000,[0,1,2,3,4,...|\n",
            "|1 54 is an annual...|https://en.wikipe...| 19|[1, 54, is, an, a...|(1000,[0,1,2,3,4,...|\n",
            "+--------------------+--------------------+---+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fit/train an LSH Model"
      ],
      "metadata": {
        "id": "5n6CaBKZXDoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from  pyspark.ml.feature import MinHashLSH\n",
        "\n",
        "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashValues\", numHashTables=3)\n",
        "LSHmodel = mh.fit(vectorizedDF)\n",
        "\n",
        "LSHmodel.transform(vectorizedDF).show()"
      ],
      "metadata": {
        "id": "hBHCdPaoWl8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "292b4621-b58c-4e1c-d9e0-bb2bfba9671f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+---+--------------------+--------------------+--------------------+\n",
            "|                text|                 url| id|               words|            features|          hashValues|\n",
            "+--------------------+--------------------+---+--------------------+--------------------+--------------------+\n",
            "|Nycticebus linglo...|https://en.wikipe...|  0|[nycticebus, ling...|(1000,[0,1,2,3,4,...|[[1081553.0], [36...|\n",
            "|Oryzomys pliocaen...|https://en.wikipe...|  1|[oryzomys, plioca...|(1000,[0,1,2,3,4,...|[[5.4265679E7], [...|\n",
            "|.hack dt hk is a ...|https://en.wikipe...|  2|[.hack, dt, hk, i...|(1000,[0,1,2,3,4,...|[[1081553.0], [36...|\n",
            "|The You Drive Me ...|https://en.wikipe...|  3|[the, you, drive,...|(1000,[0,1,2,3,4,...|[[1081553.0], [36...|\n",
            "|0 8 4 is the seco...|https://en.wikipe...|  4|[0, 8, 4, is, the...|(1000,[0,1,2,3,4,...|[[1081553.0], [1....|\n",
            "|I Corps is a corp...|https://en.wikipe...|  5|[i, corps, is, a,...|(1000,[0,1,2,3,4,...|[[1081553.0], [36...|\n",
            "|The 1982 Florida ...|https://en.wikipe...|  6|[the, 1982, flori...|(1000,[0,1,2,3,4,...|[[1081553.0], [1....|\n",
            "|Tropical Depressi...|https://en.wikipe...|  7|[tropical, depres...|(1000,[0,1,2,3,4,...|[[1.1073486E7], [...|\n",
            "|Tropical Depressi...|https://en.wikipe...|  8|[tropical, depres...|(1000,[0,1,2,3,4,...|[[2.6775095E7], [...|\n",
            "|Tropical Depressi...|https://en.wikipe...|  9|[tropical, depres...|(1000,[0,1,2,3,4,...|[[1.3928324E7], [...|\n",
            "|On 1 November 194...|https://en.wikipe...| 10|[on, 1, november,...|(1000,[0,1,2,3,4,...|[[1081553.0], [4....|\n",
            "|1 1 is a song rec...|https://en.wikipe...| 11|[1, 1, is, a, son...|(1000,[0,1,2,3,4,...|[[2508972.0], [36...|\n",
            "|1 Thing is a song...|https://en.wikipe...| 12|[1, thing, is, a,...|(1000,[0,1,2,3,4,...|[[8218648.0], [62...|\n",
            "|One Times Square,...|https://en.wikipe...| 13|[one, times, squa...|(1000,[0,1,2,3,4,...|[[2508972.0], [3....|\n",
            "|1 vs. 100 is an A...|https://en.wikipe...| 14|[1, vs., 100, is,...|(1000,[0,1,2,3,4,...|[[1081553.0], [62...|\n",
            "|One World Trade C...|https://en.wikipe...| 15|[one, world, trad...|(1000,[0,1,2,3,4,...|[[1081553.0], [17...|\n",
            "|The A1 in London ...|https://en.wikipe...| 16|[the, a1, in, lon...|(1000,[0,1,2,3,4,...|[[1081553.0], [36...|\n",
            "|Tropical Depressi...|https://en.wikipe...| 17|[tropical, depres...|(1000,[0,1,2,3,4,...|[[1081553.0], [17...|\n",
            "|1 Nenokkadine Eng...|https://en.wikipe...| 18|[1, nenokkadine, ...|(1000,[0,1,2,3,4,...|[[1081553.0], [1....|\n",
            "|1 54 is an annual...|https://en.wikipe...| 19|[1, 54, is, an, a...|(1000,[0,1,2,3,4,...|[[1.1073486E7], [...|\n",
            "+--------------------+--------------------+---+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Searching the similar pairs/items for a key \"united\" \"states\""
      ],
      "metadata": {
        "id": "zItyQZ4-YASX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(cvModel.vocabulary.index(\"united\"))\n",
        "print(cvModel.vocabulary.index(\"states\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE_xRWD4Ah0f",
        "outputId": "62fb5da3-f483-4087-aa85-e93a1f1c3c8f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92\n",
            "165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing searching for \"united\" \"states\"\n",
        "\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "\n",
        "# Convert the input with 3 words into 1000 size vectors\n",
        "# If the words exist in the index we will give value = 1.0, otherwise 0.0\n",
        "# Final result: key = [0, 0, ... , 1.0, ..., 1.0, 1.0, ....]\n",
        "\n",
        "key = Vectors.sparse(vocabSize, {cvModel.vocabulary.index(\"civil\"): 1.0, cvModel.vocabulary.index(\"war\"): 1.0})"
      ],
      "metadata": {
        "id": "i9AaCBF_XzK7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of neighbours\n",
        "k = 40\n",
        "\n",
        "# Search inside LSH model that we already trained\n",
        "resultDF = LSHmodel.approxNearestNeighbors(vectorizedDF, key, k)\n",
        "resultDF.show()"
      ],
      "metadata": {
        "id": "-EE3IJn1R77O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87284c5a-f242-47df-b165-d21accd79e44"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+---+-----+--------+----------+-------+\n",
            "|text|url| id|words|features|hashValues|distCol|\n",
            "+----+---+---+-----+--------+----------+-------+\n",
            "+----+---+---+-----+--------+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the result into CSV\n",
        "import pandas as pd\n",
        "\n",
        "data = resultDF.toPandas()\n",
        "data.to_csv(\"result.csv\")"
      ],
      "metadata": {
        "id": "3x38VwRbdJGY"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}